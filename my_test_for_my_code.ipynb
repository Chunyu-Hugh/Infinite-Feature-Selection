{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # VGG16 requires 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from model.VGG16 import VGG16\n",
    "# net = VGG16(num_classes=1000)\n",
    "# net.to(device)\n",
    "net = models.vgg16(num_classes=1000)\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"vgg16_cifar10.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"vgg16_cifar10.pth\"))\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 量化感知训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.vgg16(pretrained=False)\n",
    "model = model.to('cpu')\n",
    "\n",
    "model.load_state_dict(torch.load(\"vgg16_cifar10.pth\"))\n",
    "\n",
    "# Define qconfig and fuse the model\n",
    "qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model.qconfig = qconfig\n",
    "model = torch.quantization.prepare_qat(model, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# Using the same training loop as before\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 200 == 199:\n",
    "            print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 200:.3f}\")\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model = torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "torch.jit.save(torch.jit.script(model), \"vgg16_cifar10_quantized.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  对比量化前后的运行时间和内存占用率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 测试函数\n",
    "def test_model(model, dataloader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            \n",
    "            # images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return (100 * correct / total), elapsed_time\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载非量化模型\n",
    "model_non_quantized = models.vgg16(pretrained=False)\n",
    "model_non_quantized.load_state_dict(torch.load(\"vgg16_cifar10.pth\"))\n",
    "model_non_quantized = model_non_quantized.to(device)\n",
    "model_non_quantized.eval()\n",
    "# 加载量化模型\n",
    "model_quantized = torch.jit.load(\"vgg16_cifar10_quantized.pth\",map_location=torch.device('cpu'))\n",
    "model_quantized.eval()\n",
    "\n",
    "# 对比运行时间和内存\n",
    "accuracy_non_quantized, time_non_quantized = test_model(model_non_quantized, testloader, device)\n",
    "accuracy_quantized, time_quantized = test_model(model_quantized, testloader, device)\n",
    "\n",
    "print(f\"Non-Quantized Model - Accuracy: {accuracy_non_quantized:.2f}% | Time: {time_non_quantized:.2f} seconds\")\n",
    "print(f\"Quantized Model - Accuracy: {accuracy_quantized:.2f}% | Time: {time_quantized:.2f} seconds\")\n",
    "\n",
    "# 对比模型内存占用\n",
    "import sys\n",
    "size_non_quantized = sys.getsizeof(torch.save(model_non_quantized.state_dict(), \"tmp.pth\"))\n",
    "size_quantized = sys.getsizeof(torch.save(model_quantized, \"tmp_quantized.pth\"))\n",
    "\n",
    "print(f\"Size of Non-Quantized Model: {size_non_quantized / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Size of Quantized Model: {size_quantized / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Non-Quantized Model - Accuracy: {accuracy_non_quantized:.2f}% | Time: {time_non_quantized:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 加载JIT模型\n",
    "model = torch.jit.load('vgg16_cifar10_quantized.pth')\n",
    "\n",
    "# 检查模型的一个参数的设备\n",
    "device = next(model.parameters()).device\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载量化模型\n",
    "model_quantized = torch.jit.load(\"vgg16_cifar10_quantized.pth\",map_location=\"cpu\")\n",
    "model_quantized.eval()\n",
    "accuracy_quantized, time_quantized = test_model(model_quantized, testloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugh/anaconda3/envs/chunyu/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hugh/anaconda3/envs/chunyu/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Model...\n",
      "Epoch: -1 Eval Loss: 2.325 Eval Acc: 0.098\n",
      "Epoch: 000 Train Loss: 2.030 Train Acc: 0.318 Eval Loss: 1.588 Eval Acc: 0.420\n",
      "Epoch: 001 Train Loss: 1.491 Train Acc: 0.451 Eval Loss: 1.376 Eval Acc: 0.494\n",
      "Epoch: 002 Train Loss: 1.298 Train Acc: 0.532 Eval Loss: 1.138 Eval Acc: 0.595\n",
      "Epoch: 003 Train Loss: 1.160 Train Acc: 0.586 Eval Loss: 1.112 Eval Acc: 0.610\n",
      "Epoch: 004 Train Loss: 1.061 Train Acc: 0.623 Eval Loss: 0.985 Eval Acc: 0.653\n",
      "Epoch: 005 Train Loss: 0.979 Train Acc: 0.652 Eval Loss: 0.983 Eval Acc: 0.658\n",
      "Epoch: 006 Train Loss: 0.912 Train Acc: 0.681 Eval Loss: 0.883 Eval Acc: 0.694\n",
      "Epoch: 007 Train Loss: 0.878 Train Acc: 0.692 Eval Loss: 0.796 Eval Acc: 0.726\n",
      "Epoch: 008 Train Loss: 0.818 Train Acc: 0.713 Eval Loss: 0.778 Eval Acc: 0.729\n",
      "Epoch: 009 Train Loss: 0.778 Train Acc: 0.725 Eval Loss: 0.829 Eval Acc: 0.723\n",
      "Epoch: 010 Train Loss: 0.749 Train Acc: 0.738 Eval Loss: 0.775 Eval Acc: 0.729\n",
      "Epoch: 011 Train Loss: 0.718 Train Acc: 0.749 Eval Loss: 0.734 Eval Acc: 0.754\n",
      "Epoch: 012 Train Loss: 0.692 Train Acc: 0.759 Eval Loss: 0.731 Eval Acc: 0.751\n",
      "Epoch: 013 Train Loss: 0.676 Train Acc: 0.765 Eval Loss: 0.695 Eval Acc: 0.762\n",
      "Epoch: 014 Train Loss: 0.654 Train Acc: 0.774 Eval Loss: 0.751 Eval Acc: 0.742\n",
      "Epoch: 015 Train Loss: 0.636 Train Acc: 0.777 Eval Loss: 0.645 Eval Acc: 0.780\n",
      "Epoch: 016 Train Loss: 0.621 Train Acc: 0.784 Eval Loss: 0.635 Eval Acc: 0.781\n",
      "Epoch: 017 Train Loss: 0.602 Train Acc: 0.792 Eval Loss: 0.684 Eval Acc: 0.767\n",
      "Epoch: 018 Train Loss: 0.595 Train Acc: 0.794 Eval Loss: 0.635 Eval Acc: 0.788\n",
      "Epoch: 019 Train Loss: 0.577 Train Acc: 0.800 Eval Loss: 0.643 Eval Acc: 0.781\n",
      "Epoch: 020 Train Loss: 0.565 Train Acc: 0.803 Eval Loss: 0.620 Eval Acc: 0.789\n",
      "Epoch: 021 Train Loss: 0.556 Train Acc: 0.806 Eval Loss: 0.637 Eval Acc: 0.785\n",
      "Epoch: 022 Train Loss: 0.542 Train Acc: 0.810 Eval Loss: 0.604 Eval Acc: 0.798\n",
      "Epoch: 023 Train Loss: 0.536 Train Acc: 0.812 Eval Loss: 0.654 Eval Acc: 0.777\n",
      "Epoch: 024 Train Loss: 0.528 Train Acc: 0.818 Eval Loss: 0.607 Eval Acc: 0.795\n",
      "Epoch: 025 Train Loss: 0.523 Train Acc: 0.819 Eval Loss: 0.590 Eval Acc: 0.799\n",
      "Epoch: 026 Train Loss: 0.512 Train Acc: 0.824 Eval Loss: 0.605 Eval Acc: 0.799\n",
      "Epoch: 027 Train Loss: 0.506 Train Acc: 0.824 Eval Loss: 0.579 Eval Acc: 0.806\n",
      "Epoch: 028 Train Loss: 0.503 Train Acc: 0.826 Eval Loss: 0.623 Eval Acc: 0.796\n",
      "Epoch: 029 Train Loss: 0.497 Train Acc: 0.827 Eval Loss: 0.615 Eval Acc: 0.796\n",
      "Epoch: 030 Train Loss: 0.488 Train Acc: 0.830 Eval Loss: 0.614 Eval Acc: 0.795\n",
      "Epoch: 031 Train Loss: 0.486 Train Acc: 0.831 Eval Loss: 0.603 Eval Acc: 0.796\n",
      "Epoch: 032 Train Loss: 0.473 Train Acc: 0.836 Eval Loss: 0.548 Eval Acc: 0.814\n",
      "Epoch: 033 Train Loss: 0.464 Train Acc: 0.839 Eval Loss: 0.585 Eval Acc: 0.806\n",
      "Epoch: 034 Train Loss: 0.460 Train Acc: 0.840 Eval Loss: 0.582 Eval Acc: 0.805\n",
      "Epoch: 035 Train Loss: 0.456 Train Acc: 0.840 Eval Loss: 0.588 Eval Acc: 0.808\n",
      "Epoch: 036 Train Loss: 0.455 Train Acc: 0.839 Eval Loss: 0.620 Eval Acc: 0.799\n",
      "Epoch: 037 Train Loss: 0.452 Train Acc: 0.844 Eval Loss: 0.620 Eval Acc: 0.795\n",
      "Epoch: 038 Train Loss: 0.446 Train Acc: 0.843 Eval Loss: 0.639 Eval Acc: 0.799\n",
      "Epoch: 039 Train Loss: 0.449 Train Acc: 0.843 Eval Loss: 0.654 Eval Acc: 0.789\n",
      "Epoch: 040 Train Loss: 0.435 Train Acc: 0.848 Eval Loss: 0.594 Eval Acc: 0.806\n",
      "Epoch: 041 Train Loss: 0.436 Train Acc: 0.848 Eval Loss: 0.570 Eval Acc: 0.810\n",
      "Epoch: 042 Train Loss: 0.433 Train Acc: 0.850 Eval Loss: 0.596 Eval Acc: 0.805\n",
      "Epoch: 043 Train Loss: 0.430 Train Acc: 0.848 Eval Loss: 0.626 Eval Acc: 0.797\n",
      "Epoch: 044 Train Loss: 0.423 Train Acc: 0.853 Eval Loss: 0.579 Eval Acc: 0.813\n",
      "Epoch: 045 Train Loss: 0.422 Train Acc: 0.854 Eval Loss: 0.590 Eval Acc: 0.811\n",
      "Epoch: 046 Train Loss: 0.419 Train Acc: 0.854 Eval Loss: 0.601 Eval Acc: 0.806\n",
      "Epoch: 047 Train Loss: 0.415 Train Acc: 0.857 Eval Loss: 0.650 Eval Acc: 0.790\n",
      "Epoch: 048 Train Loss: 0.407 Train Acc: 0.858 Eval Loss: 0.563 Eval Acc: 0.813\n",
      "Epoch: 049 Train Loss: 0.406 Train Acc: 0.858 Eval Loss: 0.540 Eval Acc: 0.820\n",
      "Epoch: 050 Train Loss: 0.406 Train Acc: 0.858 Eval Loss: 0.599 Eval Acc: 0.805\n",
      "Epoch: 051 Train Loss: 0.400 Train Acc: 0.862 Eval Loss: 0.531 Eval Acc: 0.829\n",
      "Epoch: 052 Train Loss: 0.395 Train Acc: 0.863 Eval Loss: 0.568 Eval Acc: 0.819\n",
      "Epoch: 053 Train Loss: 0.397 Train Acc: 0.861 Eval Loss: 0.566 Eval Acc: 0.814\n",
      "Epoch: 054 Train Loss: 0.395 Train Acc: 0.860 Eval Loss: 0.557 Eval Acc: 0.820\n",
      "Epoch: 055 Train Loss: 0.392 Train Acc: 0.863 Eval Loss: 0.563 Eval Acc: 0.818\n",
      "Epoch: 056 Train Loss: 0.382 Train Acc: 0.867 Eval Loss: 0.577 Eval Acc: 0.813\n",
      "Epoch: 057 Train Loss: 0.387 Train Acc: 0.865 Eval Loss: 0.548 Eval Acc: 0.825\n",
      "Epoch: 058 Train Loss: 0.385 Train Acc: 0.865 Eval Loss: 0.548 Eval Acc: 0.824\n",
      "Epoch: 059 Train Loss: 0.381 Train Acc: 0.867 Eval Loss: 0.671 Eval Acc: 0.792\n",
      "Epoch: 060 Train Loss: 0.379 Train Acc: 0.868 Eval Loss: 0.557 Eval Acc: 0.822\n",
      "Epoch: 061 Train Loss: 0.378 Train Acc: 0.868 Eval Loss: 0.546 Eval Acc: 0.821\n",
      "Epoch: 062 Train Loss: 0.379 Train Acc: 0.868 Eval Loss: 0.517 Eval Acc: 0.829\n",
      "Epoch: 063 Train Loss: 0.376 Train Acc: 0.869 Eval Loss: 0.541 Eval Acc: 0.823\n",
      "Epoch: 064 Train Loss: 0.379 Train Acc: 0.867 Eval Loss: 0.572 Eval Acc: 0.820\n",
      "Epoch: 065 Train Loss: 0.374 Train Acc: 0.868 Eval Loss: 0.617 Eval Acc: 0.812\n",
      "Epoch: 066 Train Loss: 0.371 Train Acc: 0.870 Eval Loss: 0.586 Eval Acc: 0.812\n",
      "Epoch: 067 Train Loss: 0.371 Train Acc: 0.870 Eval Loss: 0.551 Eval Acc: 0.826\n",
      "Epoch: 068 Train Loss: 0.366 Train Acc: 0.873 Eval Loss: 0.582 Eval Acc: 0.817\n",
      "Epoch: 069 Train Loss: 0.360 Train Acc: 0.875 Eval Loss: 0.528 Eval Acc: 0.826\n",
      "Epoch: 070 Train Loss: 0.360 Train Acc: 0.874 Eval Loss: 0.549 Eval Acc: 0.828\n",
      "Epoch: 071 Train Loss: 0.363 Train Acc: 0.873 Eval Loss: 0.549 Eval Acc: 0.823\n",
      "Epoch: 072 Train Loss: 0.365 Train Acc: 0.871 Eval Loss: 0.593 Eval Acc: 0.808\n",
      "Epoch: 073 Train Loss: 0.359 Train Acc: 0.876 Eval Loss: 0.561 Eval Acc: 0.823\n",
      "Epoch: 074 Train Loss: 0.353 Train Acc: 0.876 Eval Loss: 0.574 Eval Acc: 0.818\n",
      "Epoch: 075 Train Loss: 0.357 Train Acc: 0.874 Eval Loss: 0.542 Eval Acc: 0.831\n",
      "Epoch: 076 Train Loss: 0.355 Train Acc: 0.875 Eval Loss: 0.524 Eval Acc: 0.830\n",
      "Epoch: 077 Train Loss: 0.351 Train Acc: 0.877 Eval Loss: 0.560 Eval Acc: 0.824\n",
      "Epoch: 078 Train Loss: 0.356 Train Acc: 0.877 Eval Loss: 0.660 Eval Acc: 0.803\n",
      "Epoch: 079 Train Loss: 0.345 Train Acc: 0.880 Eval Loss: 0.555 Eval Acc: 0.828\n",
      "Epoch: 080 Train Loss: 0.347 Train Acc: 0.879 Eval Loss: 0.588 Eval Acc: 0.817\n",
      "Epoch: 081 Train Loss: 0.352 Train Acc: 0.876 Eval Loss: 0.542 Eval Acc: 0.827\n",
      "Epoch: 082 Train Loss: 0.349 Train Acc: 0.878 Eval Loss: 0.541 Eval Acc: 0.827\n",
      "Epoch: 083 Train Loss: 0.348 Train Acc: 0.877 Eval Loss: 0.532 Eval Acc: 0.832\n",
      "Epoch: 084 Train Loss: 0.352 Train Acc: 0.876 Eval Loss: 0.534 Eval Acc: 0.831\n",
      "Epoch: 085 Train Loss: 0.347 Train Acc: 0.879 Eval Loss: 0.555 Eval Acc: 0.824\n",
      "Epoch: 086 Train Loss: 0.343 Train Acc: 0.880 Eval Loss: 0.560 Eval Acc: 0.818\n",
      "Epoch: 087 Train Loss: 0.338 Train Acc: 0.880 Eval Loss: 0.552 Eval Acc: 0.832\n",
      "Epoch: 088 Train Loss: 0.341 Train Acc: 0.881 Eval Loss: 0.621 Eval Acc: 0.812\n",
      "Epoch: 089 Train Loss: 0.337 Train Acc: 0.880 Eval Loss: 0.530 Eval Acc: 0.828\n",
      "Epoch: 090 Train Loss: 0.341 Train Acc: 0.883 Eval Loss: 0.548 Eval Acc: 0.830\n",
      "Epoch: 091 Train Loss: 0.334 Train Acc: 0.883 Eval Loss: 0.508 Eval Acc: 0.836\n",
      "Epoch: 092 Train Loss: 0.335 Train Acc: 0.882 Eval Loss: 0.569 Eval Acc: 0.816\n",
      "Epoch: 093 Train Loss: 0.341 Train Acc: 0.881 Eval Loss: 0.529 Eval Acc: 0.833\n",
      "Epoch: 094 Train Loss: 0.331 Train Acc: 0.884 Eval Loss: 0.573 Eval Acc: 0.817\n",
      "Epoch: 095 Train Loss: 0.333 Train Acc: 0.884 Eval Loss: 0.567 Eval Acc: 0.822\n",
      "Epoch: 096 Train Loss: 0.336 Train Acc: 0.882 Eval Loss: 0.541 Eval Acc: 0.824\n",
      "Epoch: 097 Train Loss: 0.337 Train Acc: 0.882 Eval Loss: 0.565 Eval Acc: 0.819\n",
      "Epoch: 098 Train Loss: 0.332 Train Acc: 0.882 Eval Loss: 0.532 Eval Acc: 0.829\n",
      "Epoch: 099 Train Loss: 0.332 Train Acc: 0.885 Eval Loss: 0.559 Eval Acc: 0.828\n",
      "Epoch: 100 Train Loss: 0.219 Train Acc: 0.924 Eval Loss: 0.439 Eval Acc: 0.864\n",
      "Epoch: 101 Train Loss: 0.175 Train Acc: 0.939 Eval Loss: 0.440 Eval Acc: 0.868\n",
      "Epoch: 102 Train Loss: 0.164 Train Acc: 0.942 Eval Loss: 0.448 Eval Acc: 0.868\n",
      "Epoch: 103 Train Loss: 0.148 Train Acc: 0.949 Eval Loss: 0.449 Eval Acc: 0.869\n",
      "Epoch: 104 Train Loss: 0.141 Train Acc: 0.950 Eval Loss: 0.457 Eval Acc: 0.870\n",
      "Epoch: 105 Train Loss: 0.134 Train Acc: 0.953 Eval Loss: 0.464 Eval Acc: 0.870\n",
      "Epoch: 106 Train Loss: 0.130 Train Acc: 0.954 Eval Loss: 0.470 Eval Acc: 0.870\n",
      "Epoch: 107 Train Loss: 0.125 Train Acc: 0.956 Eval Loss: 0.473 Eval Acc: 0.869\n",
      "Epoch: 108 Train Loss: 0.121 Train Acc: 0.958 Eval Loss: 0.472 Eval Acc: 0.870\n",
      "Epoch: 109 Train Loss: 0.115 Train Acc: 0.959 Eval Loss: 0.480 Eval Acc: 0.870\n",
      "Epoch: 110 Train Loss: 0.112 Train Acc: 0.960 Eval Loss: 0.486 Eval Acc: 0.869\n",
      "Epoch: 111 Train Loss: 0.107 Train Acc: 0.961 Eval Loss: 0.488 Eval Acc: 0.873\n",
      "Epoch: 112 Train Loss: 0.106 Train Acc: 0.962 Eval Loss: 0.491 Eval Acc: 0.873\n",
      "Epoch: 113 Train Loss: 0.101 Train Acc: 0.965 Eval Loss: 0.493 Eval Acc: 0.871\n",
      "Epoch: 114 Train Loss: 0.100 Train Acc: 0.965 Eval Loss: 0.491 Eval Acc: 0.873\n",
      "Epoch: 115 Train Loss: 0.094 Train Acc: 0.967 Eval Loss: 0.496 Eval Acc: 0.871\n",
      "Epoch: 116 Train Loss: 0.092 Train Acc: 0.967 Eval Loss: 0.500 Eval Acc: 0.871\n",
      "Epoch: 117 Train Loss: 0.089 Train Acc: 0.969 Eval Loss: 0.509 Eval Acc: 0.871\n",
      "Epoch: 118 Train Loss: 0.084 Train Acc: 0.970 Eval Loss: 0.525 Eval Acc: 0.868\n",
      "Epoch: 119 Train Loss: 0.083 Train Acc: 0.970 Eval Loss: 0.525 Eval Acc: 0.869\n",
      "Epoch: 120 Train Loss: 0.083 Train Acc: 0.971 Eval Loss: 0.525 Eval Acc: 0.874\n",
      "Epoch: 121 Train Loss: 0.082 Train Acc: 0.971 Eval Loss: 0.530 Eval Acc: 0.872\n",
      "Epoch: 122 Train Loss: 0.082 Train Acc: 0.971 Eval Loss: 0.532 Eval Acc: 0.871\n",
      "Epoch: 123 Train Loss: 0.077 Train Acc: 0.973 Eval Loss: 0.528 Eval Acc: 0.871\n",
      "Epoch: 124 Train Loss: 0.075 Train Acc: 0.973 Eval Loss: 0.526 Eval Acc: 0.870\n",
      "Epoch: 125 Train Loss: 0.074 Train Acc: 0.974 Eval Loss: 0.537 Eval Acc: 0.870\n",
      "Epoch: 126 Train Loss: 0.070 Train Acc: 0.974 Eval Loss: 0.544 Eval Acc: 0.868\n",
      "Epoch: 127 Train Loss: 0.071 Train Acc: 0.975 Eval Loss: 0.548 Eval Acc: 0.869\n",
      "Epoch: 128 Train Loss: 0.073 Train Acc: 0.974 Eval Loss: 0.557 Eval Acc: 0.868\n",
      "Epoch: 129 Train Loss: 0.069 Train Acc: 0.975 Eval Loss: 0.556 Eval Acc: 0.871\n",
      "Epoch: 130 Train Loss: 0.067 Train Acc: 0.977 Eval Loss: 0.575 Eval Acc: 0.869\n",
      "Epoch: 131 Train Loss: 0.065 Train Acc: 0.977 Eval Loss: 0.559 Eval Acc: 0.869\n",
      "Epoch: 132 Train Loss: 0.064 Train Acc: 0.978 Eval Loss: 0.559 Eval Acc: 0.872\n",
      "Epoch: 133 Train Loss: 0.063 Train Acc: 0.978 Eval Loss: 0.582 Eval Acc: 0.869\n",
      "Epoch: 134 Train Loss: 0.062 Train Acc: 0.979 Eval Loss: 0.572 Eval Acc: 0.869\n",
      "Epoch: 135 Train Loss: 0.065 Train Acc: 0.976 Eval Loss: 0.577 Eval Acc: 0.868\n",
      "Epoch: 136 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.568 Eval Acc: 0.871\n",
      "Epoch: 137 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.565 Eval Acc: 0.873\n",
      "Epoch: 138 Train Loss: 0.059 Train Acc: 0.978 Eval Loss: 0.582 Eval Acc: 0.870\n",
      "Epoch: 139 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.586 Eval Acc: 0.871\n",
      "Epoch: 140 Train Loss: 0.057 Train Acc: 0.980 Eval Loss: 0.588 Eval Acc: 0.869\n",
      "Epoch: 141 Train Loss: 0.054 Train Acc: 0.981 Eval Loss: 0.594 Eval Acc: 0.869\n",
      "Epoch: 142 Train Loss: 0.055 Train Acc: 0.981 Eval Loss: 0.577 Eval Acc: 0.874\n",
      "Epoch: 143 Train Loss: 0.057 Train Acc: 0.980 Eval Loss: 0.587 Eval Acc: 0.869\n",
      "Epoch: 144 Train Loss: 0.052 Train Acc: 0.982 Eval Loss: 0.602 Eval Acc: 0.870\n",
      "Epoch: 145 Train Loss: 0.055 Train Acc: 0.980 Eval Loss: 0.592 Eval Acc: 0.870\n",
      "Epoch: 146 Train Loss: 0.052 Train Acc: 0.982 Eval Loss: 0.611 Eval Acc: 0.866\n",
      "Epoch: 147 Train Loss: 0.051 Train Acc: 0.982 Eval Loss: 0.594 Eval Acc: 0.870\n",
      "Epoch: 148 Train Loss: 0.053 Train Acc: 0.982 Eval Loss: 0.594 Eval Acc: 0.868\n",
      "Epoch: 149 Train Loss: 0.051 Train Acc: 0.982 Eval Loss: 0.598 Eval Acc: 0.873\n",
      "Epoch: 150 Train Loss: 0.043 Train Acc: 0.985 Eval Loss: 0.583 Eval Acc: 0.876\n",
      "Epoch: 151 Train Loss: 0.037 Train Acc: 0.988 Eval Loss: 0.583 Eval Acc: 0.875\n",
      "Epoch: 152 Train Loss: 0.037 Train Acc: 0.988 Eval Loss: 0.579 Eval Acc: 0.875\n",
      "Epoch: 153 Train Loss: 0.036 Train Acc: 0.988 Eval Loss: 0.583 Eval Acc: 0.877\n",
      "Epoch: 154 Train Loss: 0.035 Train Acc: 0.988 Eval Loss: 0.581 Eval Acc: 0.875\n",
      "Epoch: 155 Train Loss: 0.035 Train Acc: 0.988 Eval Loss: 0.581 Eval Acc: 0.876\n",
      "Epoch: 156 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.587 Eval Acc: 0.874\n",
      "Epoch: 157 Train Loss: 0.032 Train Acc: 0.990 Eval Loss: 0.585 Eval Acc: 0.874\n",
      "Epoch: 158 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.587 Eval Acc: 0.877\n",
      "Epoch: 159 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.587 Eval Acc: 0.875\n",
      "Epoch: 160 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.593 Eval Acc: 0.874\n",
      "Epoch: 161 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.595 Eval Acc: 0.875\n",
      "Epoch: 162 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.597 Eval Acc: 0.875\n",
      "Epoch: 163 Train Loss: 0.028 Train Acc: 0.990 Eval Loss: 0.596 Eval Acc: 0.875\n",
      "Epoch: 164 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.602 Eval Acc: 0.875\n",
      "Epoch: 165 Train Loss: 0.028 Train Acc: 0.990 Eval Loss: 0.597 Eval Acc: 0.875\n",
      "Epoch: 166 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.600 Eval Acc: 0.875\n",
      "Epoch: 167 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.606 Eval Acc: 0.876\n",
      "Epoch: 168 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.604 Eval Acc: 0.876\n",
      "Epoch: 169 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.608 Eval Acc: 0.877\n",
      "Epoch: 170 Train Loss: 0.027 Train Acc: 0.992 Eval Loss: 0.608 Eval Acc: 0.875\n",
      "Epoch: 171 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.610 Eval Acc: 0.875\n",
      "Epoch: 172 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.614 Eval Acc: 0.874\n",
      "Epoch: 173 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.612 Eval Acc: 0.875\n",
      "Epoch: 174 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.616 Eval Acc: 0.875\n",
      "Epoch: 175 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.616 Eval Acc: 0.875\n",
      "Epoch: 176 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.620 Eval Acc: 0.872\n",
      "Epoch: 177 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.621 Eval Acc: 0.874\n",
      "Epoch: 178 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.619 Eval Acc: 0.874\n",
      "Epoch: 179 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.620 Eval Acc: 0.873\n",
      "Epoch: 180 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.617 Eval Acc: 0.874\n",
      "Epoch: 181 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.623 Eval Acc: 0.874\n",
      "Epoch: 182 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.622 Eval Acc: 0.874\n",
      "Epoch: 183 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.625 Eval Acc: 0.875\n",
      "Epoch: 184 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.628 Eval Acc: 0.874\n",
      "Epoch: 185 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.629 Eval Acc: 0.874\n",
      "Epoch: 186 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.629 Eval Acc: 0.873\n",
      "Epoch: 187 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.632 Eval Acc: 0.875\n",
      "Epoch: 188 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.634 Eval Acc: 0.875\n",
      "Epoch: 189 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.635 Eval Acc: 0.873\n",
      "Epoch: 190 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.639 Eval Acc: 0.874\n",
      "Epoch: 191 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.635 Eval Acc: 0.874\n",
      "Epoch: 192 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.637 Eval Acc: 0.873\n",
      "Epoch: 193 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.639 Eval Acc: 0.873\n",
      "Epoch: 194 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.640 Eval Acc: 0.873\n",
      "Epoch: 195 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.646 Eval Acc: 0.873\n",
      "Epoch: 196 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.646 Eval Acc: 0.873\n",
      "Epoch: 197 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.647 Eval Acc: 0.873\n",
      "Epoch: 198 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.652 Eval Acc: 0.873\n",
      "Epoch: 199 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.652 Eval Acc: 0.874\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Fusion only for eval!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=384'>385</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mINT8 JIT CPU Inference Latency: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m ms / sample\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(int8_jit_cpu_inference_latency \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=386'>387</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=388'>389</a>\u001b[0m     main()\n",
      "\u001b[1;32m/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=301'>302</a>\u001b[0m fused_model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=303'>304</a>\u001b[0m \u001b[39m# Fuse the model in place rather manually.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=304'>305</a>\u001b[0m fused_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mquantization\u001b[39m.\u001b[39;49mfuse_modules(fused_model, [[\u001b[39m\"\u001b[39;49m\u001b[39mconv1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mbn1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m]], inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=305'>306</a>\u001b[0m \u001b[39mfor\u001b[39;00m module_name, module \u001b[39min\u001b[39;00m fused_model\u001b[39m.\u001b[39mnamed_children():\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bschoolwsl/home/hugh/Infinite-Feature-Selection/my_test_for_my_code.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=306'>307</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m module_name:\n",
      "File \u001b[0;32m~/anaconda3/envs/chunyu/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:158\u001b[0m, in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuse_modules\u001b[39m(model, modules_to_fuse, inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, fuser_func\u001b[39m=\u001b[39mfuse_known_modules, fuse_custom_config_dict\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Fuses a list of modules into a single module\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    Fuses only the following sequence of modules:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m _fuse_modules(\n\u001b[1;32m    159\u001b[0m         model,\n\u001b[1;32m    160\u001b[0m         modules_to_fuse,\n\u001b[1;32m    161\u001b[0m         is_qat\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    162\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m    163\u001b[0m         fuser_func\u001b[39m=\u001b[39;49mfuser_func,\n\u001b[1;32m    164\u001b[0m         fuse_custom_config_dict\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/chunyu/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:100\u001b[0m, in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[39m# Handle case of modules_to_fuse being a list of lists\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m module_list \u001b[39min\u001b[39;00m modules_to_fuse:\n\u001b[0;32m--> 100\u001b[0m         _fuse_modules_helper(model, module_list, is_qat, fuser_func, fuse_custom_config_dict)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/chunyu/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:84\u001b[0m, in \u001b[0;36m_fuse_modules_helper\u001b[0;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     81\u001b[0m     mod_list\u001b[39m.\u001b[39mappend(_get_module(model, item))\n\u001b[1;32m     83\u001b[0m \u001b[39m# Fuse list of modules\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m new_mod_list \u001b[39m=\u001b[39m fuser_func(mod_list, is_qat, additional_fuser_method_mapping)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Replace original module list with fused module list\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(modules_to_fuse):\n",
      "File \u001b[0;32m~/anaconda3/envs/chunyu/lib/python3.11/site-packages/torch/ao/quantization/fuse_modules.py:56\u001b[0m, in \u001b[0;36mfuse_known_modules\u001b[0;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot fuse modules: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(types))\n\u001b[1;32m     55\u001b[0m new_mod : List[Optional[nn\u001b[39m.\u001b[39mModule]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(mod_list)\n\u001b[0;32m---> 56\u001b[0m fused \u001b[39m=\u001b[39m fuser_method(is_qat, \u001b[39m*\u001b[39;49mmod_list)\n\u001b[1;32m     57\u001b[0m \u001b[39m# NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# Move pre forward hooks of the base module to resulting fused module\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m handle_id, pre_hook_fn \u001b[39min\u001b[39;00m mod_list[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/anaconda3/envs/chunyu/lib/python3.11/site-packages/torch/ao/quantization/fuser_method_mappings.py:96\u001b[0m, in \u001b[0;36mfuse_conv_bn_relu\u001b[0;34m(is_qat, conv, bn, relu)\u001b[0m\n\u001b[1;32m     94\u001b[0m fused_module \u001b[39m=\u001b[39m map_to_fused_module_eval\u001b[39m.\u001b[39mget(\u001b[39mtype\u001b[39m(conv), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m fused_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     fused_conv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mfusion\u001b[39m.\u001b[39;49mfuse_conv_bn_eval(conv, bn)\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m fused_module(fused_conv, relu)\n\u001b[1;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chunyu/lib/python3.11/site-packages/torch/nn/utils/fusion.py:7\u001b[0m, in \u001b[0;36mfuse_conv_bn_eval\u001b[0;34m(conv, bn, transpose)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuse_conv_bn_eval\u001b[39m(conv, bn, transpose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39massert\u001b[39;00m(\u001b[39mnot\u001b[39;00m (conv\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m bn\u001b[39m.\u001b[39mtraining)), \u001b[39m\"\u001b[39m\u001b[39mFusion only for eval!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     fused_conv \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(conv)\n\u001b[1;32m     10\u001b[0m     fused_conv\u001b[39m.\u001b[39mweight, fused_conv\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \\\n\u001b[1;32m     11\u001b[0m         fuse_conv_bn_weights(fused_conv\u001b[39m.\u001b[39mweight, fused_conv\u001b[39m.\u001b[39mbias,\n\u001b[1;32m     12\u001b[0m                              bn\u001b[39m.\u001b[39mrunning_mean, bn\u001b[39m.\u001b[39mrunning_var, bn\u001b[39m.\u001b[39meps, bn\u001b[39m.\u001b[39mweight, bn\u001b[39m.\u001b[39mbias, transpose)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Fusion only for eval!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "    return model\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)\n",
    "\n",
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave\n",
    "\n",
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model(num_classes=10):\n",
    "\n",
    "    # The number of channels in ResNet18 is divisible by 8.\n",
    "    # This is required for fast GEMM integer matrix multiplication.\n",
    "    # model = torchvision.models.resnet18(pretrained=False)\n",
    "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
    "\n",
    "    # We would use the pretrained ResNet18 as a feature extractor.\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Modify the last FC layer\n",
    "    # num_features = model.fc.in_features\n",
    "    # model.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "    return model\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedResNet18, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "\n",
    "    random_seed = 0\n",
    "    num_classes = 10\n",
    "    cuda_device = torch.device(\"cuda:0\")\n",
    "    cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "    model_dir = \"saved_models\"\n",
    "    model_filename = \"resnet18_cifar10.pt\"\n",
    "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "    # Create an untrained model.\n",
    "    model = create_model(num_classes=num_classes)\n",
    "\n",
    "    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
    "    \n",
    "    # Train model.\n",
    "    print(\"Training Model...\")\n",
    "    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=200)\n",
    "    # Save model.\n",
    "    save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
    "    # Load a pretrained model.\n",
    "    model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
    "    # Move the model to CPU since static quantization does not support CUDA currently.\n",
    "    model.to(cpu_device)\n",
    "    # Make a copy of the model for layer fusion\n",
    "    fused_model = copy.deepcopy(model)\n",
    "\n",
    "    model.train()\n",
    "    # The model has to be switched to training mode before any layer fusion.\n",
    "    # Otherwise the quantization aware training will not work correctly.\n",
    "    fused_model.eval()\n",
    "\n",
    "    # Fuse the model in place rather manually.\n",
    "    fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "    for module_name, module in fused_model.named_children():\n",
    "        if \"layer\" in module_name:\n",
    "            for basic_block_name, basic_block in module.named_children():\n",
    "                torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "                for sub_block_name, sub_block in basic_block.named_children():\n",
    "                    if sub_block_name == \"downsample\":\n",
    "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
    "\n",
    "    # Print FP32 model.\n",
    "    print(model)\n",
    "    # Print fused model.\n",
    "    print(fused_model)\n",
    "\n",
    "    # Model and fused model should be equivalent.\n",
    "    model.eval()\n",
    "    fused_model.eval()\n",
    "    assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "    # Prepare the model for quantization aware training. This inserts observers in\n",
    "    # the model that will observe activation tensors during calibration.\n",
    "    quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
    "    # Using un-fused model will fail.\n",
    "    # Because there is no quantized layer implementation for a single batch normalization layer.\n",
    "    # quantized_model = QuantizedResNet18(model_fp32=model)\n",
    "    # Select quantization schemes from \n",
    "    # https://pytorch.org/docs/stable/quantization-support.html\n",
    "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    # Custom quantization configurations\n",
    "    # quantization_config = torch.quantization.default_qconfig\n",
    "    # quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
    "\n",
    "    quantized_model.qconfig = quantization_config\n",
    "    \n",
    "    # Print quantization configurations\n",
    "    print(quantized_model.qconfig)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
    "    torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
    "\n",
    "    # # Use training data for calibration.\n",
    "    print(\"Training QAT Model...\")\n",
    "    quantized_model.train()\n",
    "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)\n",
    "    quantized_model.to(cpu_device)\n",
    "\n",
    "    # Using high-level static quantization wrapper\n",
    "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
    "    # quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
    "\n",
    "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "    quantized_model.eval()\n",
    "\n",
    "    # Print quantized model.\n",
    "    print(quantized_model)\n",
    "\n",
    "    # Save quantized model.\n",
    "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
    "\n",
    "    # Load quantized model.\n",
    "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
    "\n",
    "    _, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "    _, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "    # Skip this assertion since the values might deviate a lot.\n",
    "    # assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "\n",
    "    print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
    "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "    fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    \n",
    "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "    print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "    print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
